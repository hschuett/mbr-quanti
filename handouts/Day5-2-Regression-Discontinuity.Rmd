---
title: "Quantitative Methods -- Day 5: Regression Discontinuity"
author: "Harm H. Schuett"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: ../literature/bibliography.bib
output: 
  html_notebook: 
    code_folding: none
    number_sections: yes
    toc: yes
---

```{r}
library(dplyr)
library(lfe)      # for FE models with instruments
library(stargazer)
```

# Sharp regression discontinuity and ignorability

> Regression discontinuity analysis is an approach dealing with this extreme case of lack of overlap in which the assignment mechanism is clearly defined. @Gelman.2007, p. 212.

Sometimes we have situations where a pre-treatment variable $x$ completely determines treatment. In particular, think of a regulation that kicks in after $x$ has reached a certain value. For example a reporting regulation that only applies to firms with a size $x$ greater than some value $C$. So, we know exactly what determines the treatment. On the other hand, this also means, that we have complete lack of overlap for the $x$ that causes treatment and likely also affect the outcome $y$.

In such a setting we can only perform a causal analysis in a region around $C$ and only if we are willing to assume that the regression function ($E[y|x]$) is a continuous function near $C$. In this case we have the advantage that we know the assignment mechanism, so we know exactly our confounders. But the disadvantage is that since there is no overlap, we must extrapolate much more what the counterfactuals might look like. And to mitigate such extrapolations, we usually restrict out attention to a small range around $C$, where we can reasonably assume that $E[y^0|x]$ would be very similar before and after $C$. Here is why:

Suppose $E[y^0|x]$, the outcome without treatment as a function of $x$ has the following form.

$$E[y^0_i|x_i] = a_0 + a_1\times x_i$$
$$y^1_i = y^0_i + \delta$$

which gives as a regression like:

$$y_i = a_0 + a_1 x_i + \delta D_i + u_i$$

where $D_i = 1$ if $x > C$ and $0$ otherwise. The important difference to previous discussions is that here $D_i$ is completely determined by $x$ (not just merely correlated). In the linear case, this is not a problem. But, what if the relation might reasonably be non-linear. Since we have a sharp discontinuity, we need to make very strong assumptions about functional form.

For instance, what is the right form here in the following data set?

```{r}
rdd_example <- read.csv("../data/rdd-example.csv")
# Fitting two types of regressions
fit1 <- lm(y~x + d, data=rdd_example)  # linear
fit2 <- lm(y~x + I(x^2) + d, data=rdd_example)  # quadratic

# Plotting both fits to the data:
par(mfrow=c(2,1), mar=c(4,4,1,1), pch=20)
# Plot 1
plot(rdd_example$x, rdd_example$y)
lines(x=rdd_example$x, y=fit1$fitted, col="blue", type="p")
abline(v=0, col="gray")
# Plot 2
plot(rdd_example$x, rdd_example$y)
lines(x=rdd_example$x, y=fit2$fitted, col="red", type="p")
abline(v=0, col="gray")
```

or with differing trends

```{r}
# Plotting both fits to the data:
par(mfrow=c(2,1), mar=c(4,4,1,1), pch=20)
fit3 <- lm(y~x + d + x:d, data=rdd_example)  # linear
fit4 <- lm(y~x + I(x^2) + d + x:d + I(x^2):d, data=rdd_example)  # quadratic
# Plot 1
plot(rdd_example$x, rdd_example$y)
lines(x=rdd_example$x, y=fit3$fitted, col="blue", type="p")
abline(v=0, col="gray")
# Plot 2
plot(rdd_example$x, rdd_example$y)
lines(x=rdd_example$x, y=fit4$fitted, col="red", type="p")
abline(v=0, col="gray")
```

Think about this a bit. What is the problem here? The bottom one could fit better. But are you sure? It could also be the top graph just with a lot of noise and the bottom just fitting better by accident.

With such a sharp case, we really need strong assumptions about what the underlying process for $E[y^0|x]$ is to guides us here, at least for a small enough region around the threshold. A bit as in the matching case, depending on the sample size and theoretic considerations, we can set the neighborhood around the threshold small enough so that functional form should not matter much.

# Fuzzy Regression discontinuity is IV

We have a different situation if the threshold is not sharp. Instead, we assume that there is a jump in the probability of treatment at a certain threshold. But it can be treated and non-treated on both sides of the threshold. 

$$P[T_i=1|x_i] = g_1(x_i) \,\,\, if \,\, X_i > C$$
$$P[T_i=1|x_i] = g_0(x_i) \,\,\, if \,\, X_i \leq C$$

In this case, we can think of a dummy variable $C_i = 1(x_i > C)$ as an instrument for the actual treatment status $T_i$. In that case we can use a two stage least squares approach just as in the IV setting.

# Example: Angrist and Lavy (1999)

@Angrist.1999 is an early study framing fuzzy regression discontinuity in a instrumental variable setting. Let's look at the study for a while and then we'll replicate some main results. Below is the code for the replication:

For comparisons, this is the Stata code for table5.do on the dataverse site:

    replace avgverb= avgverb-100 if avgverb>100
    replace avgmath= avgmath-100 if avgmath>100
    
    g func1= c_size/(int((c_size-1)/40)+1)
    g func2= cohsize/(int(cohsize/40)+1)
    
    replace avgverb=. if verbsize==0
    replace passverb=. if verbsize==0
    
    replace avgmath=. if mathsize==0
    replace passmath=. if mathsize==0
    
    keep if 1<classize & classize<45 & c_size>5
    keep if c_leom==1 & c_pik<3
    keep if avgverb~=.
    
    g byte disc= (c_size>=36 & c_size<=45) | (c_size>=76 & c_size<=85) | ///
    	(c_size>=116 & c_size<=125)
    
    g byte all=1
    g c_size2= (c_size^2)/100
    
    * GENERATE TREND
    g trend= c_size if c_size>=0 & c_size<=40
    	replace trend= 20+(c_size/2) if c_size>=41 & c_size<=80
    	replace trend= (100/3)+(c_size/3) if c_size>=81 & c_size<=120
    	replace trend= (130/3)+(c_size/4) if c_size>=121 & c_size<=160
    
    	
    mmoulton avgverb (classize=func1) tipuach c_size c_size2, clu(schlcode) 2sls
    
    foreach dvar in avgverb avgmath {
    
    di " "
    di "OUTCOME IS `dvar'"
    di "FULL SAMPLE"
    di " "
    mmoulton `dvar' (classize=func1) tipu, clu(schlcode) 2sls
    mmoulton `dvar' (classize=func1) tipu c_size, clu(schlcode) 2sls
    mmoulton `dvar' (classize=func1) tipu c_size c_size2, clu(schlcode) 2sls
    mmoulton `dvar' (classize=func1) trend, clu(schlcode) 2sls
    
    di " "
    di "OUTCOME IS `dvar'"
    di "DISCONTINUITY SAMPLE"
    di " "
    mmoulton `dvar' (classize=func1) tipu if disc==1, clu(schlcode) 2sls
    mmoulton `dvar' (classize=func1) tipu c_size if disc==1, clu(schlcode) 2sls

Let's load the data

```{r}
raw_graders5 <- haven::read_dta("../data/final5.dta")
raw_graders5 <- haven::zap_formats(raw_graders5)
```

Filter the data
```{r}
graders5 <- raw_graders5 %>% 
  mutate(avgverb = if_else(avgverb > 100, avgverb-100, avgverb),
         avgmath = if_else(avgmath > 100, avgmath-100, avgmath)) %>% 
  mutate(func1 = c_size/(floor((c_size-1)/40)+1)) %>%   # note: using round instead gives crazy results
  mutate(avgverb = if_else(verbsize== 0, NA_real_, avgverb),
         avgmath = if_else(mathsize== 0, NA_real_, avgmath)) %>% 
  filter(classize > 1 & classize < 45 & c_size > 5) %>% 
  filter(c_leom == 1 & c_pik < 3) %>% 
  filter(is.na(avgverb) == FALSE) %>% 
  mutate(disc = if_else((c_size>=36 & c_size<=45) | (c_size>=76 & c_size<=85) | (c_size>=116 & c_size<=125), 1, 0)) %>% 
  mutate(c_size2 = (c_size^2)/100)
```


## Normal OLS estimates (compare Table 2)

```{r}
f_fit1 <- lm(avgmath ~ tipuach + classize, data=graders5)
f_fit2 <- lm(avgmath ~ tipuach + c_size + classize, data=graders5)
f_fit3 <- lm(avgmath ~ tipuach + c_size + c_size2 + classize, data=graders5)
stargazer(f_fit1, f_fit2, f_fit3, type="text", omit.stat = c("f", "ser"))
```

## 2SLS estimates (compare Table 4)

```{r}
frd_fit1 <- felm(avgmath ~ tipuach | 0 | (classize ~ func1) | schlcode, data=graders5)
frd_fit2 <- felm(avgmath ~ tipuach + c_size | 0 | (classize ~ func1) | schlcode, data=graders5)
frd_fit3 <- felm(avgmath ~ tipuach + c_size + c_size2 | 0 | (classize ~ func1) | schlcode, data=graders5)
stargazer(frd_fit1, frd_fit2, frd_fit3, type="text", omit.stat = c("f", "ser"))
```



```{r}
sub_graders5 <- filter(graders5, disc == 1)
frd_fit4 <- felm(avgmath ~ tipuach | 0 | (classize ~ func1) | schlcode, data=sub_graders5)
frd_fit5 <- felm(avgmath ~ tipuach + c_size | 0 | (classize ~ func1) | schlcode, data=sub_graders5)
stargazer(frd_fit4, frd_fit5, type="text", omit.stat = c("f", "ser"))
```


# Remarks

Of course, this is a local average treatment affect LATE. Can you exlain why?

# Excercises

1. Take a look at this [paper](http://ftp.iza.org/dp10779.pdf) and think about the identification strategy. 

# References


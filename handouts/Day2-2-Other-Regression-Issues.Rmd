---
title: "Quantitative Methods -- Day 2: Other Regression Issues"
author: "Harm H. Schuett"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: ../literature/bibliography.bib
output: 
  html_notebook: 
    code_folding: none
    number_sections: yes
    toc: yes
---


```{r}
library(ggplot2)
```

> The problem is that regression is like a demon or djinn from folktale. It answers exactly that question that you asked it. Often you don't realize though that you asked the wrong question ~ paraphrasing Richard McElreath


# Colinearity

$x$-variables are almost always correlated with each other. This makes coefficient estimates less precise. The extrem is variables which are perfectly correlated. That is equivalent to saying that one variable is a linear function of one or more of the remaining variables. In that case, OLS breaks down because you cannot compute the inverse $(X'X)^{-1}$ anymore. But it is also not surprising that it breaks down. If one variable is a linear function of other variables, then the same must go for its coefficient. That one should also be a linear function of the other coefficients. That means there are infinitely many coefficient combinations possible, no unique solution exists and OLS breaks down. Essentially, OLS does not know which variable it should allocate the effect to. Many solutions give you the same best fit. The problem is not so severe if variables are simply correlated, but a similar problem exists. Say you have two variables $x_1$ and $x_2$. Both move in tandem quite often and with $y$. When $x_1$ goes up, so does $x_2$ and $y$. Then it is hard to figure out, whether it is $x_1$ or $y$ that is associated with $y$. Essentially OLS tries to separate the impact of $x_1$ and $x_2$ from the few observations where $x_1$ and $x_2$ do not move similarly. If that does not happen often, the estimates are more uncertain. If it never happens you have perfect colinearity and OLS breaks down.

What this means in practice is that our $\hat{\beta}$ are less precisely estimated (we are more likely to get extreme estimates) and high standard errors. Remember the representation of our estimates:

$$E[\hat{\beta}] = E[\beta + (X'X)^{-1}X'u]$$

If the different $x_1$, $x_2$, $x_3$, .. variables in $X$ are correlated, $(X'X)^{-1}$ gets bigger. And you have more variation on the dart board:

```{r}
df <- data.frame(x1 = runif(100, min=0, max=40),
                 y1 = runif(100, min=0, max=40))
ggplot(data=df, aes(x1, y1)) +
  geom_point() +
  coord_polar() +
  scale_y_continuous(limits=c(0, 40)) +
  scale_x_continuous(limits=c(0, 40))+
  labs(subtitle="Unbiased / High Variance") +
  theme_bw()
```

## Examples

Maybe this is easiest to see via simulation:

```{r}
set.seed(666)
n <- 100
x1 <- rnorm(n=n, mean=1, sd=2)
x2a <- rnorm(n=n, mean=3, sd=2)
x2b <- x2a + 2.9 * x1
u <- rnorm(n=n, mean=0, sd=2)
ya <- 1 + 2*x1 - 2*x2a + u
yb <- 1 + 2*x1 - 2*x2b + u
fit_a <- lm(ya ~ x1 + x2a)
fit_b <- lm(yb ~ x1 + x2b)
summary(fit_a)
summary(fit_b)
```




# Measurement Error

## The problem
A general probelm you should be aware of is measurement error. Many of our constructs and concepts (sentiment, earnings quality, risk aversion, ability, financial constraints, marginal tax rate, happines, customer satisfaction,etc.) are unobservable or hard to measure. We can try to find or create proxies. However, these will invariably measure our construct of interest with error. Think about all the textual analysis measures for sentiment, readability etc. For example, using the number of negative words as a measure of text tone. Obviously, this will not capture every nuance; it will contain significant measurement error of the real tone of the text. 

Depending on the amount of measurement error, proxies are like a broad sword as compared to a scalpel. It is fine for research questions where you are after broad effects. But you have to be careful with questions that require a scalpel. In those cases, you really need to be concerned about measurement error. 

## Mathematical presentation of the problem

There is a good treatment in @Wooldridge.2010 pp.70. The following is a short summary.

Imagine a regression of the form $y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x^*_3 + u$, but we cannot observe $x^*_3$. But we have a measure $x_3$. For example, think of $x^*_3$ as happiness and $x_3$ as a measure derived from survey questions. 

We assume $x_3$ measures $x^*_3$ with error $e_3 = x_3 - x^*_3$. We substitute this equation into the above formula and get:

$$y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + (u - a_3e_3)$$

The measurement error moves into the error term. Because the measurement error lurks in the error term, you get something very similar to a correlated omitted variables problem, if the measurement error is correlated with $x_3$. In quite a few cases this is the case. 

Let's assume the average measurement error is zero $E[e_3]=0$ (not a very conequential assumption because of the intercept in our regressions). The **classical errors-in-variables (CEV)** assumption is that the measurement error is uncorrelated with the true unobserved variable:

$$cov(x^*_3, e_3)=0$$

If we write $x_3 = x^*_3 + e_3$ (the measure is the true variable plus error) and $cov(x^*_3, e_3)=0$, this means that $cov(x_3, e_3)$ must be non-zero:

$$cov(x_3, e_3) = E[x_3e_3] = E[(x^*_3+e_3)e_3] = E[x^*_3e_3] + E[e^2_3] = \sigma^2_{e_3}$$

since $E[x^*_3e_3] = 0$ by the CEV assumption. Thus $cov(x_3, e_3)=\sigma^2_{e_3}$ the variance of the measurement error. Now, because the measurement error lurks in the error term, you get something very similar to a correlated omitted variables problem. The correlation of $x_3$ with the error term is thus $-a_3\sigma^2_{e_3}$.

In this case the regression of $y$ on $x_1$, $x_2$, and $x_3$ and $x^*_3$ is correlated with the other variables in the regression, we generally have inconsistent estimators of **all** estimators of all coefficients and it is hard to figure out the bias. if $x^*_3$ is uncorrelated with $x_1$ and $x_2$ then so is $x_3$. Then only $a_3$ is inconsistent. The bias for $a_3$ is **always an attenuation bias** because the correlation of $x_3$ with the error term is thus $-a_3\sigma^2_{e_3}$

## Examples

colGPA ¼ b0 þb1 famincþb2hsGPAþb3SAT þv 

## Excercises

1. What happens if we assume the flipside of the CEV?



# References
